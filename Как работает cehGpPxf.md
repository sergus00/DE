У нас есть класс, который выгружает данные из источника не через JDBC в Spark, а через Greenplum и PXF: мы создаём внешнюю writable-таблицу в GP, которая пишет в HDFS Parquet. Дальше нарезаем выгрузку на куски по предикатам (диапазоны дат/чисел), последовательно вставляем в эту external-таблицу — в итоге в стейдж-каталоге появляются parquet-файлы. После этого Spark просто читает их, при необходимости конвертирует timestamp в UTC и добавляет служебные поля. Временные таблицы и файлы мы чистим, прогресс логируем, а метрики считаем отдельным шагом. Такой подход даёт предсказуемую нагрузку, колоночный формат на выходе и более быстрый Spark-рид, чем тянуть всё по JDBC.